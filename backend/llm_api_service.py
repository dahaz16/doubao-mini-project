#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
============================================================================
LLM API Service (Responses API ç»Ÿä¸€å°è£…)
============================================================================

ç»Ÿä¸€å°è£… Responses API è°ƒç”¨ï¼Œæ”¯æŒï¼š
- Session Caching
- Stream / Non-stream
- JSON Output Mode
- Token æ¶ˆè€—è®°å½•

æ ¹æ®ã€ŠæœåŠ¡ç«¯æµç¨‹æ–‡æ¡£ä¸æ•°æ®åº“ç»“æ„è®¾è®¡ v3.3ã€‹ä¸­çš„ LLM API è°ƒç”¨å‚æ•°è§„èŒƒã€‚
"""

import os
import time
import logging
from typing import Optional, Dict, Any, Generator, List, AsyncGenerator
from datetime import datetime, timezone
from volcenginesdkarkruntime import AsyncArk
from .database import get_db_connection
from .config_manager import get_config

logging.basicConfig(level=logging.INFO)


# ============================================================================
# å®¢æˆ·ç«¯åˆå§‹åŒ–
# ============================================================================

def _get_ark_client() -> AsyncArk:
    """è·å– Ark å¼‚æ­¥å®¢æˆ·ç«¯"""
    api_key = os.getenv("ARK_API_KEY")
    if not api_key:
        raise ValueError("ARK_API_KEY ç¯å¢ƒå˜é‡æœªé…ç½®")
    
    return AsyncArk(
        base_url="https://ark.cn-beijing.volces.com/api/v3",
        api_key=api_key
    )


def _get_model_info(model_id: int) -> Dict[str, Any]:
    """ä» base_models è¡¨è·å–æ¨¡å‹ä¿¡æ¯"""
    with get_db_connection() as conn:
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT model_id, model_name_cn, api_model_id, input_price, output_price, cache_discount
                FROM base_models
                WHERE model_id = %s
            """, (model_id,))
            row = cursor.fetchone()
            
            if not row:
                raise ValueError(f"Model ID {model_id} ä¸å­˜åœ¨")
            
            return {
                'model_id': row[0],
                'model_name_cn': row[1],
                'api_model_id': row[2],
                'input_price': float(row[3]) if row[3] else 0,
                'output_price': float(row[4]) if row[4] else 0,
                'cache_discount': float(row[5]) if row[5] else 0.5,
            }


# ============================================================================
# Intv Agent LLM è°ƒç”¨ (æµå¼, Session Caching)
# ============================================================================

async def call_intv_llm_stream(
    user_id: str,
    input_messages: List[Dict[str, str]],
    previous_response_id: Optional[str] = None,
    expire_at: Optional[int] = None,
    temperature: float = None,
    llm_input_str: Optional[str] = None,
    related_original_text_id: Optional[int] = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Intv Agent LLM è°ƒç”¨ï¼ˆæµå¼è¾“å‡ºï¼‰- å¼‚æ­¥ç‰ˆæœ¬
    
    PRD äºŒ.2 å‚æ•°æ˜ å°„:
    - Caching: Enabled (Session æ¨¡å¼)
    - Stream: True
    - System Prompt æ”¾åœ¨ input æ•°ç»„ç¬¬ä¸€æ¡
    
    Args:
        user_id: ç”¨æˆ· ID
        input_messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "system/user/assistant", "content": "..."}]
        previous_response_id: ä¸Šä¸€è½®çš„ response_idï¼ˆå»¶ç»­ Session æ—¶ä¼ å…¥ï¼‰
        expire_at: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆUnix æ—¶é—´æˆ³ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä»é…ç½®è·å–
    
    Yields:
        dict:
            - {"type": "response_id", "response_id": "xxx"}
            - {"type": "text", "content": "xxx"}
            - {"type": "usage", "usage": {...}}
            - {"type": "done", "response_id": "xxx"}
            - {"type": "error", "message": "xxx"}
    """
    start_time = time.time()
    
    try:
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_id = int(get_config('intv_llm_model', default=1))
        model_info = _get_model_info(model_id)
        
        if temperature is None:
            temperature = float(get_config('intv_llm_temp', default=1.0))
        
        if expire_at is None:
            expire_duration = int(get_config('intv_llm_session_expire_duration', default=3600))
            expire_at = int(time.time()) + expire_duration
        
        client = _get_ark_client()
        
        # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆä¸¥æ ¼æŒ‰ç…§ PRDï¼‰
        params = {
            "model": model_info['api_model_id'],
            "input": input_messages,
            "temperature": temperature,
            "stream": True,
            "store": True,
            "expire_at": expire_at,
            "thinking": {"type": "disabled"},  # Intv ä¸éœ€è¦æ·±åº¦æ€è€ƒ
        }
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ Caching
        enable_caching = get_config('enable_llm_caching', default='false').lower() == 'true'
        
        if enable_caching:
            # å¼€å¯ Session Caching
            params["extra_body"] = {"caching": {"type": "enabled"}}
        
        if previous_response_id and enable_caching:
            params["previous_response_id"] = previous_response_id
        
        logging.info(f"ğŸ¤ Intv LLM è°ƒç”¨: model={model_info['model_name_cn']}, caching={enable_caching}, prev_id={previous_response_id[:20] if previous_response_id else 'None'}...")
        
        # è°ƒç”¨ API (Async)
        stream = await client.responses.create(**params)
        
        response_id = None
        usage_data = None
        full_output = ""  # æ”¶é›†å®Œæ•´è¾“å‡º
        
        async for event in stream:
            # 1. æå– Response å…ƒæ•°æ® (ID / Usage)
            resp_obj = getattr(event, 'response', None)
            if resp_obj:
                if hasattr(resp_obj, 'id') and not response_id:
                    response_id = resp_obj.id
                    yield {"type": "response_id", "response_id": response_id}
                
                # æå– Usage
                usage_obj = getattr(resp_obj, 'usage', None)
                if usage_obj:
                    cached_tokens = 0
                    if hasattr(usage_obj, 'input_tokens_details'):
                        details = usage_obj.input_tokens_details
                        cached_tokens = getattr(details, 'cached_tokens', 0) or 0
                    
                    usage_data = {
                        'total_tokens': getattr(usage_obj, 'total_tokens', 0),
                        'prompt_tokens': getattr(usage_obj, 'input_tokens', 0),
                        'completion_tokens': getattr(usage_obj, 'output_tokens', 0),
                        'cached_tokens': cached_tokens,
                    }
            
            # 2. æå–æ–‡æœ¬å¢é‡å†…å®¹ (Delta)
            delta = getattr(event, 'delta', None)
            if delta:
                full_output += delta  # ç´¯ç§¯è¾“å‡º
                yield {"type": "text", "content": delta}
        
        # è®¡ç®—è€—æ—¶
        duration_ms = int((time.time() - start_time) * 1000)
        
        # è®°å½•è°ƒç”¨
        if usage_data:
            yield {"type": "usage", "usage": usage_data}
            _record_llm_usage(
                user_id=user_id,
                agent="Intv",
                model_id=model_id,
                model_name_cn=model_info['model_name_cn'],
                usage=usage_data,
                duration_ms=duration_ms,
                llm_input=llm_input_str,
                llm_output=full_output,
                related_original_text_id=related_original_text_id
            )
        
        yield {"type": "done", "response_id": response_id}
        
    except Exception as e:
        logging.error(f"âŒ Intv LLM è°ƒç”¨å¤±è´¥: {e}")
        yield {"type": "error", "message": str(e)}


# ============================================================================
# Stn Agent LLM è°ƒç”¨ (éæµå¼, JSON æ¨¡å¼, æ—  Session)
# ============================================================================

async def call_stn_llm(
    user_id: str,
    input_messages: List[Dict[str, str]],
    temperature: float = None,
    llm_input_str: Optional[str] = None
) -> Dict[str, Any]:
    """
    Stn Agent LLM è°ƒç”¨ï¼ˆéæµå¼ï¼ŒJSON è¾“å‡ºï¼‰- å¼‚æ­¥ç‰ˆæœ¬
    
    PRD äºŒ.2 å‚æ•°æ˜ å°„:
    - Caching: Disabled
    - Stream: False
    - JSON æ¨¡å¼: text.format.type = "json_object"
    - æ—  previous_response_idï¼ˆå•è½®ä»»åŠ¡ï¼‰
    
    Returns:
        dict:
            - success: bool
            - content: str (JSON å­—ç¬¦ä¸²)
            - response_id: str
            - usage: dict
            - error: str (å¦‚æœå¤±è´¥)
    """
    start_time = time.time()
    
    try:
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_id = int(get_config('stn_llm_model', default=2))
        model_info = _get_model_info(model_id)
        
        if temperature is None:
            temperature = float(get_config('stn_llm_temp', default=0.1))
        
        client = _get_ark_client()
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": model_info['api_model_id'],
            "input": input_messages,
            "temperature": temperature,
            "stream": False,
            "store": False,  # Stn ä¸éœ€è¦å­˜å‚¨
            "thinking": {"type": "disabled"},
            "text": {"format": {"type": "json_object"}},  # JSON è¾“å‡ºæ¨¡å¼
        }
        
        logging.info(f"ğŸ“ Stn LLM è°ƒç”¨: model={model_info['model_name_cn']}")
        
        # è°ƒç”¨ API (Async)
        response = await client.responses.create(**params)
        
        # è§£æå“åº”
        response_id = response.id if hasattr(response, 'id') else None
        
        # æå–æ–‡æœ¬å†…å®¹
        content = ""
        if hasattr(response, 'output') and response.output:
            for output_item in response.output:
                if hasattr(output_item, 'content') and output_item.content:
                    for content_item in output_item.content:
                        if hasattr(content_item, 'text'):
                            content += content_item.text
        
        # æå– usage
        usage_data = None
        if hasattr(response, 'usage') and response.usage:
            usage_data = {
                'total_tokens': response.usage.total_tokens,
                'prompt_tokens': getattr(response.usage, 'input_tokens', 0),
                'completion_tokens': getattr(response.usage, 'output_tokens', 0),
                'cached_tokens': 0,
            }
        
        # è®¡ç®—è€—æ—¶å¹¶è®°å½•
        duration_ms = int((time.time() - start_time) * 1000)
        
        if usage_data:
            _record_llm_usage(
                user_id=user_id,
                agent="Stn",
                model_id=model_id,
                model_name_cn=model_info['model_name_cn'],
                usage=usage_data,
                duration_ms=duration_ms,
                llm_input=llm_input_str,
                llm_output=content
            )
        
        logging.info(f"âœ… Stn LLM è°ƒç”¨æˆåŠŸ: {len(content)} å­—ç¬¦, {duration_ms}ms")
        
        return {
            "success": True,
            "content": content,
            "response_id": response_id,
            "usage": usage_data,
        }
        
    except Exception as e:
        logging.error(f"âŒ Stn LLM è°ƒç”¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
        }


# ============================================================================
# Dir Agent LLM è°ƒç”¨ (éæµå¼, Session Caching)
# ============================================================================

async def call_dir_llm(
    user_id: str,
    input_messages: List[Dict[str, str]],
    previous_response_id: Optional[str] = None,
    expire_at: Optional[int] = None,
    temperature: float = None,
    llm_input_str: Optional[str] = None
) -> Dict[str, Any]:
    """
    Dir Agent LLM è°ƒç”¨ï¼ˆéæµå¼ï¼‰- å¼‚æ­¥ç‰ˆæœ¬
    
    PRD äºŒ.2 å‚æ•°æ˜ å°„:
    - Caching: Enabled (Session æ¨¡å¼)
    - Stream: False
    
    Returns:
        dict:
            - success: bool
            - content: str
            - response_id: str
            - usage: dict
            - error: str (å¦‚æœå¤±è´¥)
    """
    start_time = time.time()
    
    try:
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_id = int(get_config('dir_llm_model', default=2))
        model_info = _get_model_info(model_id)
        
        if temperature is None:
            temperature = float(get_config('dir_llm_temp', default=0.7))
        
        if expire_at is None:
            expire_duration = int(get_config('dir_llm_session_expire_duration', default=3600))
            expire_at = int(time.time()) + expire_duration
        
        client = _get_ark_client()
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": model_info['api_model_id'],
            "input": input_messages,
            "temperature": temperature,
            "stream": False,
            "store": True,
            "expire_at": expire_at,
            "thinking": {"type": "disabled"},
        }
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ Caching
        enable_caching = get_config('enable_llm_caching', default='false').lower() == 'true'
        
        if enable_caching:
            # å¼€å¯ Session Caching
            params["extra_body"] = {"caching": {"type": "enabled"}}
        
        if previous_response_id and enable_caching:
            params["previous_response_id"] = previous_response_id
        
        logging.info(f"ğŸ¬ Dir LLM è°ƒç”¨: model={model_info['model_name_cn']}, caching={enable_caching}")
        
        # è°ƒç”¨ API (Async)
        response = await client.responses.create(**params)
        
        # è§£æå“åº”
        response_id = response.id if hasattr(response, 'id') else None
        
        # æå–æ–‡æœ¬å†…å®¹
        content = ""
        if hasattr(response, 'output') and response.output:
            for output_item in response.output:
                if hasattr(output_item, 'content') and output_item.content:
                    for content_item in output_item.content:
                        if hasattr(content_item, 'text'):
                            content += content_item.text
        
        # æå– usage
        usage_data = None
        if hasattr(response, 'usage') and response.usage:
            cached_tokens = 0
            if hasattr(response.usage, 'input_tokens_details'):
                details = response.usage.input_tokens_details
                if hasattr(details, 'cached_tokens'):
                    cached_tokens = details.cached_tokens or 0
            
            usage_data = {
                'total_tokens': response.usage.total_tokens,
                'prompt_tokens': getattr(response.usage, 'input_tokens', 0),
                'completion_tokens': getattr(response.usage, 'output_tokens', 0),
                'cached_tokens': cached_tokens,
            }
        
        # è®¡ç®—è€—æ—¶å¹¶è®°å½•
        duration_ms = int((time.time() - start_time) * 1000)
        
        if usage_data:
            _record_llm_usage(
                user_id=user_id,
                agent="Dir",
                model_id=model_id,
                model_name_cn=model_info['model_name_cn'],
                usage=usage_data,
                duration_ms=duration_ms,
                llm_input=llm_input_str,
                llm_output=content
            )
        
        logging.info(f"âœ… Dir LLM è°ƒç”¨æˆåŠŸ: {len(content)} å­—ç¬¦, {duration_ms}ms")
        
        return {
            "success": True,
            "content": content,
            "response_id": response_id,
            "usage": usage_data,
        }
        
    except Exception as e:
        logging.error(f"âŒ Dir LLM è°ƒç”¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
        }


# ============================================================================
# LLM è°ƒç”¨è®°å½•
# ============================================================================

def _record_llm_usage(
    user_id: str,
    agent: str,
    model_id: int,
    model_name_cn: str,
    usage: Dict[str, int],
    duration_ms: int,
    llm_input: Optional[str] = None,
    llm_output: Optional[str] = None,
    related_original_text_id: Optional[int] = None
):
    """è®°å½• LLM è°ƒç”¨åˆ° llm_processed è¡¨"""
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO llm_processed 
                    (user_id, agent, model_id, model_name_cn, process_duration,
                     total_tokens, prompt_tokens, completion_tokens, cached_tokens,
                     input, output, related_original_text_id)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    user_id,
                    agent,
                    model_id,
                    model_name_cn,
                    duration_ms,
                    usage.get('total_tokens', 0),
                    usage.get('prompt_tokens', 0),
                    usage.get('completion_tokens', 0),
                    usage.get('cached_tokens', 0),
                    llm_input,
                    llm_output,
                    related_original_text_id,
                ))
                conn.commit()
        
        logging.info(f"ğŸ“Š è®°å½• LLM è°ƒç”¨: {agent} - {usage.get('total_tokens', 0)} tokens")
        
    except Exception as e:
        logging.error(f"âŒ è®°å½• LLM è°ƒç”¨å¤±è´¥: {e}")
